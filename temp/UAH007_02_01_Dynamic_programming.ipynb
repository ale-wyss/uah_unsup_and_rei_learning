{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning: Policies and value functions valuation\n",
    "In this first notebook, it will be created the code to interactive policies and value functions evaluate. First, it will be created a Grid object for the game.\n",
    "\n",
    "## Gridworld¶\n",
    "The Gridworld game is a board in which an agent can move until reaching one of two final states.\n",
    "\n",
    "The board and the game can be controlled with the `Grid` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.230055Z",
     "start_time": "2019-06-12T18:10:10.125170Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Grid:\n",
    "    def __init__(self, width, height, start):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.i = start[0]\n",
    "        self.j = start[1]\n",
    "\n",
    "    # Class properties\n",
    "    possible_actions = ('U', 'D', 'L', 'R')\n",
    "\n",
    "    def set(self, rewards, actions):\n",
    "        # rewards is a dictionary: (i, j): r (row, col): rewards\n",
    "        # actions is a dictionary: (i, j): A (row, col): list of possible actions\n",
    "\n",
    "        self.rewards = rewards\n",
    "        self.actions = actions\n",
    "\n",
    "    def set_state(self, s):\n",
    "        self.i = s[0]\n",
    "        self.j = s[1]\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions[(self.i, self.j)]\n",
    "\n",
    "    def current_state(self):\n",
    "        return (self.i, self.j)\n",
    "\n",
    "    def is_terminal(self, s):\n",
    "        return s not in self.actions\n",
    "\n",
    "    def move(self, action):\n",
    "        # Check if the move is legal valid\n",
    "        if action in self.actions[(self.i, self.j)]:\n",
    "            if action == 'U':\n",
    "                self.i -= 1\n",
    "            elif action == 'D':\n",
    "                self.i += 1\n",
    "            elif action == 'R':\n",
    "                self.j += 1\n",
    "            elif action == 'L':\n",
    "                self.j -= 1\n",
    "\n",
    "        # Returns the reward\n",
    "        return self.rewards.get((self.i, self.j), 0)\n",
    "\n",
    "    def undo_move(self, action):\n",
    "        # Undo the movement\n",
    "        if action == 'U':\n",
    "            self.i += 1\n",
    "        elif action == 'D':\n",
    "            self.i -= 1\n",
    "        elif action == 'R':\n",
    "            self.j -= 1\n",
    "        elif action == 'L':\n",
    "            self.j += 1\n",
    "\n",
    "        # Throw an exception in the cell in invalid.\n",
    "        assert(self.current_state() in self.all_states())\n",
    "\n",
    "    def game_over(self):\n",
    "        # Returns true when the game is over and false otherwise.\n",
    "        return (self.i, self.j) not in self.actions\n",
    "\n",
    "    def all_states(self):\n",
    "        # Returns all states\n",
    "        return set(self.actions.keys()) | set(self.rewards.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To facilitate future work it will create a function that can create a board with the game and the possible cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.240482Z",
     "start_time": "2019-06-12T18:10:10.231716Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_grid(step_cost=0):\n",
    "    # Create a new board with the reward and possible actions in the cells.\n",
    "    # Default each movement does not have a cost.\n",
    "    #\n",
    "    # The board is as follows\n",
    "    #   S initial point\n",
    "    #   X not allowed cell\n",
    "    #   . allowed cell\n",
    "    #\n",
    "    # Number indicate the rewards of states\n",
    "    #\n",
    "    #\n",
    "    # .  .  .  1\n",
    "    # .  x  . -1\n",
    "    # s  .  .  .\n",
    "\n",
    "    grid = Grid(3, 4, (2, 0))\n",
    "\n",
    "    rewards = {\n",
    "        (0, 0): step_cost,\n",
    "        (0, 1): step_cost,\n",
    "        (0, 2): step_cost,\n",
    "        (0, 3): 1,\n",
    "        (1, 0): step_cost,\n",
    "        (1, 2): step_cost,\n",
    "        (1, 3): -1,\n",
    "        (2, 0): step_cost,\n",
    "        (2, 1): step_cost,\n",
    "        (2, 2): step_cost,\n",
    "        (2, 3): step_cost}\n",
    "\n",
    "    actions = {\n",
    "        (0, 0): ('D', 'R'),\n",
    "        (0, 1): ('L', 'R'),\n",
    "        (0, 2): ('L', 'D', 'R'),\n",
    "        (1, 0): ('U', 'D'),\n",
    "        (1, 2): ('U', 'D', 'R'),\n",
    "        (2, 0): ('U', 'R'),\n",
    "        (2, 1): ('L', 'R'),\n",
    "        (2, 2): ('L', 'R', 'U'),\n",
    "        (2, 3): ('L', 'U')\n",
    "    }\n",
    "\n",
    "    grid.set(rewards, actions)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting function are print values, print policy and print value and policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.259360Z",
     "start_time": "2019-06-12T18:10:10.243713Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_values(value, grid):\n",
    "    for i in range(grid.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(grid.height):\n",
    "            v = value.get((i, j), 0)\n",
    "            if v >= 0:\n",
    "                print(\" %.2f|\" % v, end=\"\")\n",
    "            else:\n",
    "                print(\"%.2f|\" % v, end=\"\")\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def print_policy(policy, grid):\n",
    "    for i in range(grid.width):\n",
    "        print(\"---------------------------\")\n",
    "        for j in range(grid.height):\n",
    "            action = policy.get((i, j), ' ')\n",
    "\n",
    "            if action == 'U':\n",
    "                print(\"  ↑  |\", end=\"\")\n",
    "            elif action == 'D':\n",
    "                print(\"  ↓  |\", end=\"\")\n",
    "            elif action == 'R':\n",
    "                print(\"  →  |\", end=\"\")\n",
    "            elif action == 'L':\n",
    "                print(\"  ←  |\", end=\"\")\n",
    "            else:\n",
    "                print(\"     |\", end=\"\")\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "def print_value_policy(V, policy, grid):\n",
    "    print(\"Value function\")\n",
    "    print_values(V, grid)\n",
    "    print()\n",
    "    print(\"Policy\")\n",
    "    print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a function to initializes the states of the value function randomly or include an input except for the final states. Final states witch are always zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.269460Z",
     "start_time": "2019-06-12T18:10:10.263354Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_states(grid, value=None):\n",
    "    states = grid.all_states()\n",
    "    V = {}\n",
    "\n",
    "    for s in states:\n",
    "        if s in grid.actions:\n",
    "            if value is None:\n",
    "                V[s] = np.random.random()\n",
    "            else:\n",
    "                V[s] = value\n",
    "        else:\n",
    "            V[s] = 0\n",
    "\n",
    "    return V, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative evaluation of value function and policy \n",
    "Now value functions and policies iteratively can be evaluated iteratively. For this, the Bellman equation will be applied until the results converge.\n",
    "\n",
    "### Value function\n",
    "The value function of the game can be calculated assuming that the decision to be taken in each of the movements is random. For this it is possible to use an iterative method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.285546Z",
     "start_time": "2019-06-12T18:10:10.271452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "-0.03| 0.09| 0.22| 0.00|\n",
      "---------------------------\n",
      "-0.16| 0.00|-0.44| 0.00|\n",
      "---------------------------\n",
      "-0.29|-0.41|-0.54|-0.77|\n"
     ]
    }
   ],
   "source": [
    "grid = create_grid()\n",
    "V, states = init_states(grid)\n",
    "\n",
    "num_iter = 0\n",
    "gamma = 1\n",
    "max_iter = 100\n",
    "threshold = 1e-3\n",
    "\n",
    "# Iterate until the convergence criterion is verified or maximum iterations is archives.\n",
    "while num_iter < max_iter:\n",
    "    num_iter += 1\n",
    "    biggest_change = 0\n",
    "\n",
    "    # Iterate over all states\n",
    "    for s in states:\n",
    "        old_v = V[s]\n",
    "\n",
    "        # It is only calculated if there is a policy for the state\n",
    "        if s in grid.actions:\n",
    "            new_v = 0\n",
    "            p_a = 1.0 / len(grid.actions[s])\n",
    "\n",
    "            for a in grid.actions[s]:\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(a)\n",
    "                new_v += p_a * (r + gamma * V[grid.current_state()])\n",
    "\n",
    "            V[s] = new_v\n",
    "\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    if biggest_change < threshold:\n",
    "        break\n",
    "\n",
    "if num_iter >= max_iter:\n",
    "    print(\"The maximum number of iterations has been reached\")\n",
    "\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "The process can be done analogously with the policies. In this case, it is necessary to define a policy and solve the Bellman equation in an iterative way for the cells where there is a defined policy.\n",
    "\n",
    "First, the policy to be evaluated must be defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.300474Z",
     "start_time": "2019-06-12T18:10:10.287666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  →  |  →  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |  →  |  →  |  ↑  |\n"
     ]
    }
   ],
   "source": [
    "policy = {\n",
    "    (2, 0): 'U',\n",
    "    (1, 0): 'U',\n",
    "    (0, 0): 'R',\n",
    "    (0, 1): 'R',\n",
    "    (0, 2): 'R',\n",
    "    (1, 2): 'R',\n",
    "    (2, 1): 'R',\n",
    "    (2, 2): 'R',\n",
    "    (2, 3): 'U'}\n",
    "\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a process similar to the previously one is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.316151Z",
     "start_time": "2019-06-12T18:10:10.302345Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 1.00| 1.00| 1.00| 0.00|\n",
      "---------------------------\n",
      " 1.00| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 1.00|-1.00|-1.00|-1.00|\n"
     ]
    }
   ],
   "source": [
    "V, states = init_states(grid)\n",
    "\n",
    "num_iter = 0\n",
    "\n",
    "# Iterate until the convergence criterion is verified or maximum iterations is archives.\n",
    "while num_iter < max_iter:\n",
    "    num_iter += 1\n",
    "    biggest_change = 0\n",
    "\n",
    "    # Iterate over all states\n",
    "    for s in states:\n",
    "        old_v = V[s]\n",
    "\n",
    "        # It is only calculated if there is a policy for the state\n",
    "        if s in policy:\n",
    "            a = policy[s]\n",
    "            grid.set_state(s)\n",
    "            r = grid.move(a)\n",
    "            V[s] = r + gamma * V[grid.current_state()]\n",
    "\n",
    "            biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "    if biggest_change < threshold:\n",
    "        break\n",
    "\n",
    "if num_iter >= max_iter:\n",
    "    print(\"The maximum number of iterations has been reached\")\n",
    "\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to note that any cell that arrives at the positive final states has a value equal to 1 and -1 where it arrives at the negative final state. This is because without a discount rate it does not matter how long it takes to reach the final state.\n",
    "\n",
    "### Code factorization\n",
    "It is a good practice to factorize repeated code. Policy evaluation will be used multiple times, so it can create the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.328424Z",
     "start_time": "2019-06-12T18:10:10.318758Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation(grid, policy, V, states, gamma=1, max_iter=100, threshold=1e-3):\n",
    "    num_iter = 0\n",
    "\n",
    "    while num_iter < max_iter:\n",
    "        num_iter += 1\n",
    "        biggest_change = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "\n",
    "            if s in policy:\n",
    "                action = policy[s]\n",
    "                grid.set_state(s)\n",
    "                r = grid.move(action)\n",
    "                V[s] = r + gamma * V[grid.current_state()]\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "        if biggest_change < threshold:\n",
    "            break\n",
    "\n",
    "    if num_iter >= max_iter:\n",
    "        print(\"The maximum number of iterations has been reached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this function it is not necessary to return the value of `V`, since this is an object and therefore it is passed by reference.\n",
    "\n",
    "Now the results obtained previously can be checked again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.338271Z",
     "start_time": "2019-06-12T18:10:10.330505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 1.00| 1.00| 1.00| 0.00|\n",
      "---------------------------\n",
      " 1.00| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 1.00|-1.00|-1.00|-1.00|\n"
     ]
    }
   ],
   "source": [
    "V, states = init_states(grid)\n",
    "policy_evaluation(grid, policy, V, states)\n",
    "\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible check that indicating a discount rate the values in the value function will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.350415Z",
     "start_time": "2019-06-12T18:10:10.341774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.66|-0.81|-0.90|-1.00|\n"
     ]
    }
   ],
   "source": [
    "V, states = init_states(grid)\n",
    "policy_evaluation(grid, policy, V, states, gamma=0.9)\n",
    "\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or when the rewards are changed. The rewards that are in use are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.367308Z",
     "start_time": "2019-06-12T18:10:10.354523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00|-1.00|\n",
      "---------------------------\n",
      " 0.00| 0.00| 0.00| 0.00|\n"
     ]
    }
   ],
   "source": [
    "print_values(grid.rewards, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be changed to penalize extra movements using a negative reward of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.381409Z",
     "start_time": "2019-06-12T18:10:10.369290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "-0.10|-0.10|-0.10| 1.00|\n",
      "---------------------------\n",
      "-0.10| 0.00|-0.10|-1.00|\n",
      "---------------------------\n",
      "-0.10|-0.10|-0.10|-0.10|\n"
     ]
    }
   ],
   "source": [
    "grid = create_grid(step_cost=-0.1)\n",
    "print_values(grid.rewards, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the value function of the policy becomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.395817Z",
     "start_time": "2019-06-12T18:10:10.383905Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      " 0.31|-1.00|-1.00|-1.00|\n"
     ]
    }
   ],
   "source": [
    "V, states = init_states(grid)\n",
    "\n",
    "policy_evaluation(grid, policy, V, states, gamma=0.9)\n",
    "\n",
    "print_values(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy optimization\n",
    "Based on the function that evaluates the policy it is possible to create a new function to optimize. For that, it is necessary to iterate over the functions to get the best policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.411299Z",
     "start_time": "2019-06-12T18:10:10.398343Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimal_policy(grid, policy, gamma=1, max_iter=100, threshold=1e-3):\n",
    "\n",
    "    # Initialize the funciton value and states\n",
    "    V, states = init_states(grid)\n",
    "\n",
    "    # Iterate over policies until convergence or maximum iterations\n",
    "    num_iter = 0\n",
    "\n",
    "    while num_iter < max_iter:\n",
    "        num_iter += 1\n",
    "\n",
    "        # Policy evaluation\n",
    "        policy_evaluation(grid, policy, V, states, gamma, max_iter, threshold)\n",
    "\n",
    "        # Improvement the policy\n",
    "        is_policy_converged = True\n",
    "        for s in states:\n",
    "            if s in policy:\n",
    "                old_a = policy[s]\n",
    "                new_a = None\n",
    "                best_value = float('-inf')\n",
    "\n",
    "                # Iterate over actions until the best is obtained\n",
    "                for a in Grid.possible_actions:\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    v = r + gamma * V[grid.current_state()]\n",
    "                    if v > best_value:\n",
    "                        best_value = v\n",
    "                        new_a = a\n",
    "                policy[s] = new_a\n",
    "\n",
    "                if new_a != old_a:\n",
    "                    is_policy_converged = False\n",
    "\n",
    "        if is_policy_converged:\n",
    "            break\n",
    "\n",
    "    if num_iter >= max_iter:\n",
    "        print(\"The maximum number of iterations has been reached\")\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, only the value function must be returned.\n",
    "\n",
    "Now, as it is necessary to have an initial policy, it can be create a function that initialize randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.417764Z",
     "start_time": "2019-06-12T18:10:10.413391Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_policy(grid):\n",
    "    policy = {}\n",
    "\n",
    "    for s in grid.actions.keys():\n",
    "        policy[s] = np.random.choice(grid.actions[s])\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: (0, 0)\n",
      "2: (0, 1)\n",
      "3: (0, 2)\n",
      "4: (1, 0)\n",
      "5: (1, 2)\n",
      "6: (2, 0)\n",
      "7: (2, 1)\n",
      "8: (2, 2)\n",
      "9: (2, 3)\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for i in grid.actions.keys():\n",
    "    j+= 1\n",
    "    print(\"{}: {}\".format(j,i))\n",
    "print(len(grid.actions.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 'R',\n",
       " (0, 1): 'R',\n",
       " (0, 2): 'D',\n",
       " (1, 0): 'U',\n",
       " (1, 2): 'D',\n",
       " (2, 0): 'R',\n",
       " (2, 1): 'L',\n",
       " (2, 2): 'R',\n",
       " (2, 3): 'U'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(random_policy(grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "With the function defined above it can create a random policy for the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.431427Z",
     "start_time": "2019-06-12T18:10:10.420548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  →  |  →  |  ←  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  ↓  |     |\n",
      "---------------------------\n",
      "  →  |  →  |  ↑  |  ↑  |\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy(grid)\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the policy can be optimized on the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.466311Z",
     "start_time": "2019-06-12T18:10:10.441469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "---------------------------\n",
      " 0.81| 0.90| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.73| 0.00| 0.90| 0.00|\n",
      "---------------------------\n",
      " 0.66| 0.73| 0.81| 0.73|\n",
      "\n",
      "Policy\n",
      "---------------------------\n",
      "  →  |  →  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  ↑  |     |\n",
      "---------------------------\n",
      "  ↑  |  →  |  ↑  |  ←  |\n"
     ]
    }
   ],
   "source": [
    "grid = create_grid()\n",
    "\n",
    "V = optimal_policy(grid, policy, gamma=0.9)\n",
    "\n",
    "print_value_policy(V, policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can check how affects increasing the cost of doing an action up to a value of -2. For that, a new random policy is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.480904Z",
     "start_time": "2019-06-12T18:10:10.472446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  →  |  →  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  ↑  |     |\n",
      "---------------------------\n",
      "  →  |  ←  |  ↑  |  ←  |\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy(grid)\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it can optimize the policy on a board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.503136Z",
     "start_time": "2019-06-12T18:10:10.483586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "---------------------------\n",
      "-2.99|-1.10| 1.00| 0.00|\n",
      "---------------------------\n",
      "-4.69| 0.00|-1.00| 0.00|\n",
      "---------------------------\n",
      "-6.15|-4.61|-2.90|-1.00|\n",
      "\n",
      "Policy\n",
      "---------------------------\n",
      "  →  |  →  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  →  |     |\n",
      "---------------------------\n",
      "  →  |  →  |  ↑  |  ↑  |\n"
     ]
    }
   ],
   "source": [
    "grid = create_grid(step_cost=-2)\n",
    "\n",
    "V = optimal_policy(grid, policy, gamma=0.9)\n",
    "\n",
    "print_value_policy(V, policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this situation it is better for the agent to go towards the final state with a reward equal to -1 than towards the final state with reward +1.\n",
    "\n",
    "## Optimizing the policy in a world with wind\n",
    "An alternative to the basic game is a board with wind. In this a percentage of times the agent movements change randoly.The wind moves the player to an unwanted state. This can be implemented simply by entering a random part in the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.520676Z",
     "start_time": "2019-06-12T18:10:10.507410Z"
    }
   },
   "outputs": [],
   "source": [
    "def policy_evaluation_windy(grid, policy, V, states, windy=0.5, gamma=1, max_iter=100, threshold=1e-3):\n",
    "    num_iter = 0\n",
    "\n",
    "    while num_iter < max_iter:\n",
    "        num_iter += 1\n",
    "        biggest_change = 0\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "            new_v = 0\n",
    "\n",
    "            if s in policy:\n",
    "                for action in grid.actions[s]:\n",
    "                    if action == policy[s]:\n",
    "                        p = windy\n",
    "                    else:\n",
    "                        p = (1-windy)/(len(grid.actions[s])-1)\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(action)\n",
    "                    new_v += p * (r + gamma * V[grid.current_state()])\n",
    "\n",
    "                V[s] = new_v\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "        if biggest_change < threshold:\n",
    "            break\n",
    "\n",
    "    if num_iter >= max_iter:\n",
    "        print(\"The maximum number of iterations has been reached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the policy optimization, this random movements must also be taken into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.529958Z",
     "start_time": "2019-06-12T18:10:10.522154Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimal_policy_windy(grid, policy, windy=0.5, gamma=1, max_iter=100, threshold=1e-3):\n",
    "\n",
    "    # Initialize the funciton value and states\n",
    "    V, states = init_states(grid)\n",
    "\n",
    "    # Iterate over policies until convergence or maximum iterations\n",
    "    num_iter = 0\n",
    "\n",
    "    while num_iter < max_iter:\n",
    "        num_iter += 1\n",
    "\n",
    "        # Policy evaluation\n",
    "        policy_evaluation_windy(grid, policy, V, states,\n",
    "                                windy, gamma, max_iter, threshold)\n",
    "\n",
    "        # Improvement the policy\n",
    "        is_policy_converged = True\n",
    "        for s in states:\n",
    "            if s in policy:\n",
    "                old_a = policy[s]\n",
    "                new_a = None\n",
    "                best_value = float('-inf')\n",
    "\n",
    "                # Iterate over actions until the best is obtained\n",
    "                for action in Grid.possible_actions:\n",
    "                    v = 0\n",
    "                    for w_action in grid.actions[s]:\n",
    "                        if action == w_action:\n",
    "                            p = windy\n",
    "                        else:\n",
    "                            p = (1-windy)/(len(grid.actions[s])-1)\n",
    "                        grid.set_state(s)\n",
    "                        r = grid.move(action)\n",
    "                        v += p * (r + gamma * V[grid.current_state()])\n",
    "                    if v > best_value:\n",
    "                        best_value = v\n",
    "                        new_a = action\n",
    "                policy[s] = new_a\n",
    "\n",
    "                if new_a != old_a:\n",
    "                    is_policy_converged = False\n",
    "\n",
    "        if is_policy_converged:\n",
    "            break\n",
    "\n",
    "    if num_iter >= max_iter:\n",
    "        print(\"The maximum number of iterations has been reached\")\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Now it is possible to repeat the previous exercise in this new world. A random policy is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.541817Z",
     "start_time": "2019-06-12T18:10:10.532616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  ↓  |  →  |  →  |     |\n",
      "---------------------------\n",
      "  ↓  |     |  →  |     |\n",
      "---------------------------\n",
      "  →  |  ←  |  →  |  ←  |\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy(grid)\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it can be optimize the policy on a board with wind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.566699Z",
     "start_time": "2019-06-12T18:10:10.545827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "---------------------------\n",
      " 0.20| 0.35| 0.57| 0.00|\n",
      "---------------------------\n",
      " 0.10| 0.00|-0.03| 0.00|\n",
      "---------------------------\n",
      " 0.01|-0.06|-0.16|-0.57|\n",
      "\n",
      "Policy\n",
      "---------------------------\n",
      "  →  |  →  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  ↑  |     |\n",
      "---------------------------\n",
      "  ↑  |  ←  |  ↑  |  ←  |\n"
     ]
    }
   ],
   "source": [
    "grid = create_grid()\n",
    "\n",
    "V = optimal_policy_windy(grid, policy, gamma=0.9)\n",
    "\n",
    "print_value_policy(V, policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be checked that the policy changes when a penalty of -1 is added in each movement. To evaluate it, a new random policy is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.578449Z",
     "start_time": "2019-06-12T18:10:10.569320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  ↓  |  ←  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  ↑  |     |\n",
      "---------------------------\n",
      "  ↑  |  →  |  ←  |  ←  |\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy(grid)\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the policy can be optimized on a board with wind and a cost of -1 for any movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.598124Z",
     "start_time": "2019-06-12T18:10:10.582034Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "---------------------------\n",
      "-5.92|-4.32|-1.47| 0.00|\n",
      "---------------------------\n",
      "-6.60| 0.00|-2.21| 0.00|\n",
      "---------------------------\n",
      "-6.53|-5.69|-3.89|-2.75|\n",
      "\n",
      "Policy\n",
      "---------------------------\n",
      "  →  |  →  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  →  |     |\n",
      "---------------------------\n",
      "  →  |  →  |  ↑  |  ↑  |\n"
     ]
    }
   ],
   "source": [
    "grid = create_grid(step_cost=-1)\n",
    "\n",
    "V = optimal_policy_windy(grid, policy, gamma=0.9)\n",
    "\n",
    "print_value_policy(V, policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as a check, if the wind equals 1, we have the results of the previous case. To evaluate it, a new random policy is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.611257Z",
     "start_time": "2019-06-12T18:10:10.600598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  →  |  ←  |  ←  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  ↑  |     |\n",
      "---------------------------\n",
      "  ↑  |  ←  |  ↑  |  ←  |\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy(grid)\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.634431Z",
     "start_time": "2019-06-12T18:10:10.616498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "\n",
      "Policy\n",
      "---------------------------\n",
      "  →  |  →  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  ↑  |     |\n",
      "---------------------------\n",
      "  ↑  |  →  |  ↑  |  ←  |\n"
     ]
    }
   ],
   "source": [
    "grid = create_grid(step_cost=-0.1)\n",
    "\n",
    "V = optimal_policy_windy(grid, policy, gamma=0.9, windy=1)\n",
    "\n",
    "print_value_policy(V, policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value function Optimization\n",
    "Alternatively, the value function can be optimized. For this it can be  created the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.647208Z",
     "start_time": "2019-06-12T18:10:10.637144Z"
    }
   },
   "outputs": [],
   "source": [
    "def optimal_value(grid, policy, gamma=0.9, max_iter=100, threshold=1e-3):\n",
    "    V, states = init_states(grid)\n",
    "\n",
    "    num_iter = 0\n",
    "    while num_iter < max_iter:\n",
    "        num_iter += 1\n",
    "        biggest_change = 0\n",
    "\n",
    "        for s in states:\n",
    "            old_v = V[s]\n",
    "\n",
    "            # La función de valor solo tiene valor si no es final\n",
    "            if s in policy:\n",
    "                new_v = float('-inf')\n",
    "                for a in grid.actions[s]:\n",
    "                    grid.set_state(s)\n",
    "                    r = grid.move(a)\n",
    "                    v = r + gamma * V[grid.current_state()]\n",
    "                    if v > new_v:\n",
    "                        new_v = v\n",
    "                V[s] = new_v\n",
    "                biggest_change = max(biggest_change, np.abs(old_v - V[s]))\n",
    "\n",
    "        if biggest_change < threshold:\n",
    "            break\n",
    "\n",
    "    if num_iter >= max_iter:\n",
    "        print(\"The maximum number of iterations has been reached\")\n",
    "\n",
    "    # Obtener la politica para la función de valor\n",
    "    for s in policy.keys():\n",
    "        best_a = None\n",
    "        best_value = float('-inf')\n",
    "        # Itera sobre todas las posibles acciones\n",
    "        for a in Grid.possible_actions:\n",
    "            grid.set_state(s)\n",
    "            r = grid.move(a)\n",
    "            v = r + gamma * V[grid.current_state()]\n",
    "            if v > best_value:\n",
    "                best_value = v\n",
    "                best_a = a\n",
    "\n",
    "        policy[s] = best_a\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "As in the previous case, a random policy is used at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.657999Z",
     "start_time": "2019-06-12T18:10:10.649372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  ↓  |  ←  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  ↓  |     |\n",
      "---------------------------\n",
      "  →  |  →  |  ←  |  ↑  |\n"
     ]
    }
   ],
   "source": [
    "policy = random_policy(grid)\n",
    "print_policy(policy, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-12T18:10:10.671786Z",
     "start_time": "2019-06-12T18:10:10.659594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function\n",
      "---------------------------\n",
      " 0.62| 0.80| 1.00| 0.00|\n",
      "---------------------------\n",
      " 0.46| 0.00| 0.80| 0.00|\n",
      "---------------------------\n",
      " 0.31| 0.46| 0.62| 0.46|\n",
      "\n",
      "Policy\n",
      "---------------------------\n",
      "  →  |  →  |  →  |     |\n",
      "---------------------------\n",
      "  ↑  |     |  ↑  |     |\n",
      "---------------------------\n",
      "  ↑  |  →  |  ↑  |  ←  |\n"
     ]
    }
   ],
   "source": [
    "grid = create_grid(step_cost=-0.1)\n",
    "\n",
    "V = optimal_value(grid, policy)\n",
    "\n",
    "print_value_policy(V, policy, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions defined in this notebook are saved in a Python file to be imported and used in subsequent notebooks."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
